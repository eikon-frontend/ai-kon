# Importation des modules n√©cessaires
# Ces modules sont des "bo√Ætes √† outils" qui permettent d'ajouter des fonctionnalit√©s √† Python
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, load_index_from_storage  # Pour g√©rer les documents et l'indexation
from llama_index.core import PromptTemplate, Settings  # Pour cr√©er des mod√®les de questions/r√©ponses et configurer le syst√®me
from llama_index.llms.openai import OpenAI  # Pour utiliser l'intelligence artificielle d'OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding  # Pour utiliser les embeddings d'OpenAI
import os  # Pour interagir avec le syst√®me de fichiers (dossiers, fichiers, etc.)
import streamlit as st  # Pour cr√©er une interface web simple

# ---------------------------------------
# CONFIGURATION et INSTALLATION
# ---------------------------------------

# Avant d'utiliser ce script, il faut installer les biblioth√®ques n√©cessaires :
# - Pour installer llama-index, tapez dans le terminal : pip install llama-index
# - Pour installer streamlit, tapez dans le terminal : pip install streamlit
#
# Pour utiliser l'intelligence artificielle d'OpenAI, il faut une cl√© API (une sorte de mot de passe) :
# - Cr√©ez un compte sur https://platform.openai.com/api-keys et copiez votre cl√©
# - Sur Mac ou Linux, tapez dans le terminal : export OPENAI_API_KEY=VOTRE_CL√â
# - Sur Windows, tapez : set OPENAI_API_KEY=VOTRE_CL√â
#
# Pour lancer l'application, tapez dans le terminal : streamlit run app.py

# Chemin du dossier o√π sont stock√©s les documents √† analyser
DATA_DIR = "./data"
# Chemin du dossier o√π sera sauvegard√© l'index (la "m√©moire" de l'IA)
INDEX_DIR = "./storage"

# V√©rification de la cl√© API OpenAI
if not os.getenv("OPENAI_API_KEY"):
    st.error("‚ö†Ô∏è Cl√© API OpenAI manquante ! Veuillez d√©finir la variable d'environnement OPENAI_API_KEY")
    st.stop()

# === INITIALISATION DU MOD√àLE LLM ===
# Nom du mod√®le d'intelligence artificielle √† utiliser (ici, un mod√®le OpenAI)
# disponibles pour ce projet: gpt-4.1-nano / gpt-4o-mini
LLM_MODEL_NAME = "gpt-4o-mini"


# On cr√©e une instance du mod√®le d'OpenAI avec le nom choisi
llm = OpenAI(model = LLM_MODEL_NAME, temperature=0.01) # 0 = tr√®s pr√©cis, peu cr√©atif // 1 = pas fiable du tout, tr√®s cr√©atif
# On indique √† llama-index d'utiliser ce mod√®le par d√©faut
Settings.llm = llm

# mod√®le de repr√©sentation de texte
# disponibles pour ce projet: text-embedding-3-small
embed_model = OpenAIEmbedding(model="text-embedding-3-small")
Settings.embed_model = embed_model

# Cette fonction permet de charger ou de cr√©er l'index des documents
@st.cache_data  # Cette ligne permet de ne pas refaire le travail si rien n'a chang√© (gain de temps)
def load_index():
    """
    Charge ou cr√©e un index √† partir des documents du r√©pertoire sp√©cifi√©.

    Si le dossier d'index n'existe pas, le syst√®me lit les documents du dossier data,
    cr√©e un nouvel index et le sauvegarde. Si le dossier d'index existe, il charge
    l'index depuis le stockage directement.
    """
    # Si le dossier d'index n'existe pas, on lit les documents et on cr√©e l'index
    if not os.path.exists(INDEX_DIR):
        documents = SimpleDirectoryReader(DATA_DIR).load_data()  # On lit tous les fichiers du dossier data
        index = VectorStoreIndex.from_documents(documents)  # On cr√©e l'index √† partir des documents
        index.storage_context.persist(persist_dir=INDEX_DIR)  # On sauvegarde l'index dans le dossier storage
    else:
        # Si l'index existe d√©j√†, on le charge pour ne pas tout refaire
        storage_context = StorageContext.from_defaults(persist_dir=INDEX_DIR)
        index = load_index_from_storage(storage_context)
    return index  # On retourne l'index (la "m√©moire" de l'IA)

# On charge l'index au d√©marrage de l'application
index = load_index()

# Cette fonction pr√©pare le mod√®le de question/r√©ponse (le "prompt")
def prepare_template():
    """
    Pr√©pare un template de prompt pour le syst√®me de questions/r√©ponses.
    """
    # Le texte ci-dessous sert de consigne √† l'IA pour r√©pondre comme on le souhaite
    text_qa_template_str = """
    Tu es AI-kon, un expert en interactive media design et tu es √† eikon, une √©cole professionnelle d'arts appliqu√©s √† Fribourg, en Suisse. Tu connais tout sur l'√©cole, son r√®glement et son fonctionnement, ainsi que sur les m√©tiers de la cr√©ation num√©rique.
    Tu r√©ponds aux questions des √©l√®ves, en les tutoyant.
    Un¬∑e √©l√®ve t'a pos√© cette question : {query_str}
    Voil√† tout ce que tu sais √† ce sujet :
    --------
    {context_str}
    --------
    D√©tecte la langue de la question pos√©e par l'√©l√®ve et r√©pond dans cette langue. Voici les r√®gles √† suivre pour r√©pondre :
    √Ä partir de ces connaissances, et uniquement √† partir d'elles. Ne r√©ponds pas √† la question si tu n'as pas d'informations pertinentes. Si tu ne sais pas, dis que tu ne sais pas.
    R√©ponds de mani√®re concise et pr√©cise, sans faire de blabla inutile. Sois amical¬∑e et engageant¬∑e, mais reste professionnel¬∑le. Utilise un langage simple et clair, sans jargon technique.
    """
    # Ne r√©ponds pas si la question porte sur un sujet qui est en dehors des connaissances de l'√©cole.
    # On pourrait ajouter une blague √† la fin de la r√©ponse (optionnel)
    # if random.random() < 0.5:
    #     text_qa_template_str += "Termine par une blague geek."
    qa_template = PromptTemplate(text_qa_template_str) # On cr√©e le template
    return qa_template  # On retourne le template

# On affiche un titre et une description sur la page web
st.markdown("""
# AI-kon

AI-kon est un assistant virtuel qui r√©pond aux questions des √©l√®ves d'eikon. Il est bas√© sur le mod√®le GPT-4o-mini de OpenAI et utilise la biblioth√®que llama-index pour g√©rer les documents et les requ√™tes. Il connait tout sur l'√©cole, son r√®glement et son fonctionnement.
"""
)

# On v√©rifie si la liste des messages existe d√©j√† dans la session (pour garder l'historique du chat)
if "messages" not in st.session_state:
    # Si ce n'est pas le cas, on l'initialise avec un message d'accueil de l'assistant
    st.session_state.messages = [{"role": "assistant", "content": "Pose tes questions sur l'√©cole, je suis l√† pour t'aider¬†!"}]

# On r√©cup√®re la question de l'utilisateur (si elle existe) et on l'ajoute √† la liste des messages
if prompt := st.chat_input("Que veux-tu savoir¬†?"):
    st.session_state.messages.append({"role": "user", "content": prompt})

# On d√©finit les avatars (ic√¥nes) pour l'assistant et l'utilisateur
assistant_avatar_filepath = "ü§ñ"
user_avatar_filepath = "üôÇ"
# On affiche tous les messages du chat avec l'avatar correspondant
for message in st.session_state.messages:
    with st.chat_message(message["role"], avatar=assistant_avatar_filepath if message["role"] == "assistant" else user_avatar_filepath):
        st.write(message["content"])

# On pr√©pare le template de question/r√©ponse
qa_template = prepare_template()
# On cr√©e un "moteur de recherche" qui va utiliser l'index et le template pour r√©pondre
query_engine = index.as_query_engine(text_qa_template=qa_template, similarity_top_k=2)

# Si le dernier message vient de l'utilisateur, l'assistant doit r√©pondre
if st.session_state.messages[-1]["role"] == "user":
    # On affiche le message de l'assistant avec l'avatar
    with st.chat_message("assistant", avatar=assistant_avatar_filepath):
        # On affiche un message d'attente pendant que l'IA r√©fl√©chit
        with st.spinner("Patientez deux secondes le temps que AI-kon se r√©veille"):
            response = query_engine.query(prompt)  # L'IA g√©n√®re une r√©ponse √† la question
        if response:
            st.markdown(response.response)  # On affiche la r√©ponse sur la page web
            # On ajoute la r√©ponse √† l'historique des messages
            st.session_state.messages.append({"role": "assistant", "content": response.response})

        # Pour voir le texte utilis√© par l'IA pour r√©pondre (optionnel, pour les curieux) :
        if hasattr(response, "source_nodes"):
            with st.expander("üìö Sources utilis√©es"):
                for node in response.source_nodes:
                    st.markdown(f"- **Document**: {node.metadata.get('file_name', 'inconnu')}")
                    st.markdown(f"  > {node.node.get_text()[:200]}...")

            

