from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, load_index_from_storage
from llama_index.core import PromptTemplate, Settings
from llama_index.llms.openai import OpenAI
import os
import streamlit as st
import random

# ---------------------------------------
# CONFIGURATION et INSTALLATION
# ---------------------------------------

# Installer llama-index :‚ÄØpip install llama-index
# Installer streamlit :‚ÄØpip install streamlit
# Placez la cl√© OpenAI dans une variable d'environnement nomm√©e OPENAI_API_KEY :
# - "export OPENAI_API_KEY=XXXXX" sur linux et macOS, "set OPENAI_API_KEY=XXXXX" sur Windows
# lancez le script avec : streamlit run app.py

DATA_DIR = "./data"
INDEX_DIR = "./storage"
LLM_MODEL_NAME = "gpt-4o-mini"

llm = OpenAI(model = LLM_MODEL_NAME)
Settings.llm = llm

@st.cache_data
def load_index():
    """
    Charge ou cr√©e un index √† partir des documents du r√©pertoire sp√©cifi√©.

    Si le dossier d'index n'existe pas, le syst√®me lit les documents du dossier data,
    cr√©e un nouvel index et le sauvegarde. Si le dossier d'index existe, il charge
    l'index depuis le stockage directement.
    """
    if not os.path.exists(INDEX_DIR):
        documents = SimpleDirectoryReader(DATA_DIR).load_data()
        index = VectorStoreIndex.from_documents(documents)
        index.storage_context.persist(persist_dir=INDEX_DIR)
    else:
        storage_context = StorageContext.from_defaults(persist_dir=INDEX_DIR)
        index = load_index_from_storage(storage_context)
    return index

index = load_index()

def prepare_template():
    """
    Pr√©pare un template de prompt pour le syst√®me de questions/r√©ponses.
    """
    text_qa_template_str = """
    Tu es AI-kon, un expert en interactive media design et tu es √† eikon, une √©cole professionnelle d'arts appliqu√©s √† Fribourg, en Suisse. Tu connais tout sur l'√©cole, son r√®glement et son fonctionnement, ainsi que sur les m√©tiers de la cr√©ation num√©rique.
    Tu r√©ponds aux questions des √©l√®ves, en les tutoyant.
    Un¬∑e √©l√®ve t'a pos√© cette question : {query_str}
    Voil√† tout ce que tu sais √† ce sujet :
    --------
    {context_str}
    --------
    √Ä partir de ces connaissances, et uniquement √† partir d‚Äôelles, r√©ponds en fran√ßais √† la question.
    R√©ponds en faisant des all√©gories et des m√©taphores alambiqu√©es, comme si tu √©tais un¬∑e po√®te¬∑esse du 19√®me si√®cle.
    """
    # Blague finale :
    # if random.random() < 0.5:
    #     text_qa_template_str += "Termine par une blague geek."
    qa_template = PromptTemplate(text_qa_template_str)
    return qa_template


st.markdown("""
# AI-kon

AI-kon est un assistant virtuel qui r√©pond aux questions des √©l√®ves d'eikon. Il est bas√© sur le mod√®le GPT-4o-mini de OpenAI et utilise la biblioth√®que llama-index pour g√©rer les documents et les requ√™tes. Il connait tout sur l'√©cole, son r√®glement et son fonctionnement.
"""
)

# Initialise les messages de session_state s'ils ne sont pas d√©j√† pr√©sents
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Pose tes questions sur l'√©cole, je suis l√† pour t'aider¬†!"}]

# Capture l'entr√©e utilisateur et l'ajoute aux messages de session_state
if prompt := st.chat_input("Que veux-tu savoir¬†?"):
    st.session_state.messages.append({"role": "user", "content": prompt})

# assistant_avatar_filepath = "media/avatar.png"
assistant_avatar_filepath = "ü§ñ"
user_avatar_filepath = "üôÇ"
# Affiche les messages du chat avec les avatars appropri√©s
for message in st.session_state.messages:
    with st.chat_message(message["role"], avatar=assistant_avatar_filepath if message["role"] == "assistant" else user_avatar_filepath):
        st.write(message["content"])


qa_template = prepare_template()
query_engine = index.as_query_engine(text_qa_template=qa_template, similarity_top_k=2)

if st.session_state.messages[-1]["role"] == "user":
    with st.chat_message("assistant", avatar=assistant_avatar_filepath):
        with st.spinner("Patientez deux secondes le temps que AI-kon se r√©veille"):
            response = query_engine.query(prompt)
        if response:
            st.markdown(response.response)
            st.session_state.messages.append({"role": "assistant", "content": response.response})

            # pour afficher le contenu utilis√© pour g√©n√©rer la r√©ponse :
            #for node in response.source_nodes:
            #    print("\n----------------")
            #    print(f"Texte utilis√© pour r√©pondre : {node.text}")

